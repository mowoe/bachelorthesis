{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07004f64-d0ba-4b75-b316-040dc1355566",
   "metadata": {},
   "source": [
    "In this notebook we will train a Variational Auto-Encoder VAE, which will (hopefully) translate all UI images into some n-dimensional space, in which we can then do the optimising.\n",
    "\n",
    "We will use [https://github.com/AntixK/PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE/tree/master) as a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b741fad-2825-48cf-995b-79cd1c2f5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 17 13:39:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              53W / 400W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f58b6d2-d352-45eb-a10e-7cac8d7a81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "from utils import calculate_initial_theta, stack_alpha_aware, transform_to_t\n",
    "import utils\n",
    "from math import prod\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcfd9f9-1e21-4899-8d63-35a55b46fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, transforms\n",
    "transform_t_to_pil = T.ToPILImage()\n",
    "transform_to_t = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b652964f-cdd0-4d7b-882b-0455e8af26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6df59-ef7b-48c1-98f6-ec8b84cb501e",
   "metadata": {},
   "source": [
    "Proposed in_channels:\n",
    "\n",
    "Almost all UIs will have 3 Buttons and 3 `FIXME` objects. So we will input an ordered list into it of the respective normalised (between 0 and 1) positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7545833-7a27-4b22-911c-14df884088db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clickable_resources(item, should_be_clickable):\n",
    "    if item is None:\n",
    "        return []\n",
    "    all_boxes = []\n",
    "    if \"bounds\" in item.keys() and \"resource-id\" in item.keys() and \"clickable\" in item.keys() and item[\"clickable\"]==should_be_clickable and item[\"visible-to-user\"]:\n",
    "        all_boxes.append((item[\"bounds\"],item[\"resource-id\"]))\n",
    "    if \"children\" in item.keys():\n",
    "        for child in item[\"children\"]:\n",
    "            for box in get_all_clickable_resources(child,should_be_clickable):\n",
    "                all_boxes.append(box)\n",
    "    return all_boxes\n",
    "\n",
    "def get_all_bounding_boxes(item, should_be_clickable):\n",
    "    bboxes = get_all_clickable_resources(item,should_be_clickable)\n",
    "    reduced_bboxes = []\n",
    "    already_seen_ids = []\n",
    "    for box,r_id in bboxes:\n",
    "        if r_id not in already_seen_ids:\n",
    "            reduced_bboxes.append(box)\n",
    "            already_seen_ids.append(r_id)\n",
    "    return reduced_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6d9f6f-e120-4d14-aca1-ebf13f89ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_UI_DIMENSIONS = (1440, 2560)\n",
    "\n",
    "def combine_list_of_lists(a,b):\n",
    "    assert len(a) == len(b)\n",
    "    lol = []\n",
    "    for x in range(len(a)):\n",
    "        assert len(a[x]) == len(a[x])\n",
    "        i = []\n",
    "        for h in range(len(a[x])):\n",
    "            i.append(a[x][h])\n",
    "        for h in range(len(b[x])):\n",
    "            i.append(b[x][h])\n",
    "        lol.append(i)\n",
    "    return lol\n",
    "\n",
    "class CustomRicoDataset(Dataset):\n",
    "    def __init__(self, combined_path=\"./combined\", return_metadata=False, forward_to_next_index_on_error=False):\n",
    "        self.image_files = [\n",
    "            f\n",
    "            for f in listdir(combined_path)\n",
    "            if isfile(join(combined_path, f)) and (\"jpg\" in f or \"jpeg\" in f)\n",
    "        ]\n",
    "        self.combined_path = combined_path\n",
    "        self.return_metadata = return_metadata\n",
    "        self.forward_to_next_index_on_error = forward_to_next_index_on_error\n",
    "\n",
    "        self.get_first_n_clickable = 4\n",
    "        self.get_first_n_non_clickable = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = join(self.combined_path,self.image_files[idx])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        json_path = join(self.combined_path,self.image_files[idx].split(\".\")[0]+\".json\")\n",
    "        with open(json_path, \"r\") as f:\n",
    "            image_json = json.load(f)\n",
    "\n",
    "        \n",
    "        if not self.forward_to_next_index_on_error:\n",
    "            clickable_elems = utils.get_first_n_sorted_elements(image, image_json, self.get_first_n_clickable, True)\n",
    "            non_clickable_elems = utils.get_first_n_sorted_elements(image, image_json, self.get_first_n_non_clickable, False)\n",
    "        else:\n",
    "            try:\n",
    "                clickable_elems = utils.get_first_n_sorted_elements(image, image_json, self.get_first_n_clickable, True)\n",
    "                non_clickable_elems = utils.get_first_n_sorted_elements(image, image_json, self.get_first_n_non_clickable, False)\n",
    "            except ValueError:\n",
    "                if idx == (len(self)-1):\n",
    "                    return self[idx-100]\n",
    "                return self[idx+1]\n",
    "                \n",
    "        combined_elems = combine_list_of_lists(clickable_elems, non_clickable_elems)\n",
    "        \n",
    "        if self.return_metadata:\n",
    "            return [combined_elems[0], torch.tensor(combined_elems[1]).unsqueeze(0), combined_elems[2]], img_path\n",
    "        else:\n",
    "            return torch.tensor(combined_elems[1]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a85ed0-0bf6-4b2e-8f12-5ec84ee1075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomRicoDataset(\"../combined\", forward_to_next_index_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d050b5c7-9ab7-4022-98c7-440bf31888b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    okay_points = 0\n",
    "    not_okay_points = 0\n",
    "    indexes = list(range(len(dataset)))\n",
    "    random.shuffle(indexes)\n",
    "    for x in tqdm(indexes):\n",
    "        try:\n",
    "            _ = dataset[sample_idx]\n",
    "            okay_points += 1\n",
    "        except ValueError:\n",
    "            print(f\"{x}: Not okay\")\n",
    "            not_okay_points += 1\n",
    "    okay_points, not_okay_points        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b742cb7-447e-4f33-b44c-c1defbe7e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "j = dataset[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbee199f-a99c-4d17-b40f-5aecaaea346d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e27aa8f-839f-4f92-b2e0-3125522cb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from .types_ import *\n",
    "\n",
    "\n",
    "class VanillaVAE(BaseVAE):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Number of elems that have been chosen previously\n",
    "        self.n_elems = 7\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                    #          kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.Linear(in_channels, h_dim),\n",
    "                    # nn.BatchNorm1d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*self.n_elems, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*self.n_elems, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * self.n_elems)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    #nn.ConvTranspose2d(hidden_dims[i],\n",
    "                    #                   hidden_dims[i + 1],\n",
    "                    #                   kernel_size=3,\n",
    "                    #                   stride = 2,\n",
    "                    #                   padding=1,\n",
    "                    #                   output_padding=1),\n",
    "                    #nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.Linear(hidden_dims[i], hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            #nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                            #                   hidden_dims[-1],\n",
    "                            #                   kernel_size=3,\n",
    "                            #                   stride=2,\n",
    "                            #                   padding=1,\n",
    "                            #                   output_padding=1),\n",
    "                            #nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.Linear(hidden_dims[-1],hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Linear(hidden_dims[-1],2),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, self.n_elems, 512)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1d2f41-09ce-43ca-962e-43ba0405f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VanillaVAE(in_channels=2, latent_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c994b6-641b-4861-8f69-bef42ab948cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons, inp, mu, log_var = vae.forward(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9872237-58a6-4faf-8027-f0ea19fe22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae.loss_function(recons,inp,mu,log_var, M_N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cbe71bb-2580-4f60-ace5-ba887fe62a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.3512, grad_fn=<AddBackward0>),\n",
       " 'Reconstruction_Loss': tensor(0.3416),\n",
       " 'KLD': tensor(-0.0096)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb9c33b0-7bfa-434a-8c6f-7e145c0e4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9099e645-f5cc-4ad0-be93-9bb3de698581",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de4da22-b783-4ff9-bd43-a836833a9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=25, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfd0f573-686a-4be3-aa86-a7cee029a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, optimizer, train_dataloader, scheduler = accelerator.prepare(vae, optimizer, train_dataloader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf56184-2cf3-44f3-b3ac-a649829e8b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=3584, out_features=32, bias=True)\n",
       "  (fc_var): Linear(in_features=3584, out_features=32, bias=True)\n",
       "  (decoder_input): Linear(in_features=32, out_features=3584, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335654a-7aa2-492e-8ad1-100db90d8e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ee03de6c04a65bff9d8f0a0242af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82534/3492591280.py:143: UserWarning: Using a target size (torch.Size([64, 1, 7, 2])) that is different to the input size (torch.Size([64, 7, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recons_loss =F.mse_loss(recons, input)\n",
      "\u001b[32m2024-06-17 13:41:10.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m1.018463373184204\u001b[0m\n",
      "\u001b[32m2024-06-17 13:41:46.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m127.92259216308594\u001b[0m\n",
      "\u001b[32m2024-06-17 13:42:19.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m6.683449745178223\u001b[0m\n",
      "\u001b[32m2024-06-17 13:42:51.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m6.616822719573975\u001b[0m\n",
      "\u001b[32m2024-06-17 13:43:24.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m2.3233423233032227\u001b[0m\n",
      "\u001b[32m2024-06-17 13:44:01.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.8615033030509949\u001b[0m\n",
      "\u001b[32m2024-06-17 13:44:32.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m1.3487515449523926\u001b[0m\n",
      "\u001b[32m2024-06-17 13:45:07.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.5231943726539612\u001b[0m\n",
      "\u001b[32m2024-06-17 13:45:45.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.541184663772583\u001b[0m\n",
      "\u001b[32m2024-06-17 13:46:15.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.6421751379966736\u001b[0m\n",
      "\u001b[32m2024-06-17 13:46:47.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.4466674327850342\u001b[0m\n",
      "\u001b[32m2024-06-17 13:47:18.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.14139814674854279\u001b[0m\n",
      "\u001b[32m2024-06-17 13:47:47.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.7677732706069946\u001b[0m\n",
      "\u001b[32m2024-06-17 13:48:17.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.5130643248558044\u001b[0m\n",
      "\u001b[32m2024-06-17 13:48:53.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.4015350639820099\u001b[0m\n",
      "\u001b[32m2024-06-17 13:49:32.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.23028446733951569\u001b[0m\n",
      "\u001b[32m2024-06-17 13:50:09.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.1407480388879776\u001b[0m\n",
      "\u001b[32m2024-06-17 13:50:41.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.07049565762281418\u001b[0m\n",
      "\u001b[32m2024-06-17 13:51:13.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.17290833592414856\u001b[0m\n",
      "\u001b[32m2024-06-17 13:51:45.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.16067767143249512\u001b[0m\n",
      "\u001b[32m2024-06-17 13:52:19.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.09483227878808975\u001b[0m\n",
      "\u001b[32m2024-06-17 13:52:50.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.11602858453989029\u001b[0m\n",
      "\u001b[32m2024-06-17 13:53:26.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.08412392437458038\u001b[0m\n",
      "\u001b[32m2024-06-17 13:53:59.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.10200189799070358\u001b[0m\n",
      "\u001b[32m2024-06-17 13:54:33.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.11037175357341766\u001b[0m\n",
      "\u001b[32m2024-06-17 13:55:02.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.05911477282643318\u001b[0m\n",
      "\u001b[32m2024-06-17 13:55:33.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.15671776235103607\u001b[0m\n",
      "\u001b[32m2024-06-17 13:56:08.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.13904286921024323\u001b[0m\n",
      "\u001b[32m2024-06-17 13:56:40.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.1092933639883995\u001b[0m\n",
      "\u001b[32m2024-06-17 13:57:11.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.13045017421245575\u001b[0m\n",
      "\u001b[32m2024-06-17 13:57:44.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.1473596692085266\u001b[0m\n",
      "\u001b[32m2024-06-17 13:58:16.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.10044611245393753\u001b[0m\n",
      "\u001b[32m2024-06-17 13:58:54.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.06649746000766754\u001b[0m\n",
      "\u001b[32m2024-06-17 13:59:25.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.12242285907268524\u001b[0m\n",
      "\u001b[32m2024-06-17 13:59:54.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.14030705392360687\u001b[0m\n",
      "\u001b[32m2024-06-17 14:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.10744227468967438\u001b[0m\n",
      "\u001b[32m2024-06-17 14:00:57.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.09772565960884094\u001b[0m\n",
      "\u001b[32m2024-06-17 14:01:29.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.12370112538337708\u001b[0m\n",
      "\u001b[32m2024-06-17 14:02:01.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.12144263088703156\u001b[0m\n",
      "\u001b[32m2024-06-17 14:02:33.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.11331740021705627\u001b[0m\n",
      "\u001b[32m2024-06-17 14:03:07.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.11127346009016037\u001b[0m\n",
      "\u001b[32m2024-06-17 14:03:37.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m0.10214339196681976\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "force_retrain = True\n",
    "losses = []\n",
    "if not isfile(\"./vae_pretrained\") or force_retrain:\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # j_tensor = .to(accelerate)\n",
    "        # print(batch)\n",
    "        recons, inp, mu, log_var = vae.forward(batch)\n",
    "        loss = vae.loss_function(recons,inp,mu,log_var, M_N=64)\n",
    "        # loss[\"loss\"].backward()\n",
    "        \n",
    "        accelerator.backward(loss[\"loss\"])\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.5)\n",
    "    \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # logger.info(f\"{loss['loss'].item()}, lr: {scheduler.get_last_lr()[0]}\")\n",
    "        logger.info(f\"{loss['loss'].item()}\")\n",
    "        losses.append(loss[\"loss\"].detach().cpu().item())\n",
    "        scheduler.step(loss[\"loss\"])\n",
    "    torch.save(vae.state_dict(), \"./vae_pretrained\")\n",
    "else:\n",
    "    vae.load_state_dict(torch.load(\"./vae_pretrained\"))\n",
    "    vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51356b11-d67e-47c3-ae53-195c40207bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cc06a-cd81-4cea-ba21-92aa2dc6e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if losses:\n",
    "    losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a55af5-ed21-48ca-9d7f-ec37d2905632",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc24c9-b913-43f8-a36d-a61b4bceb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons, inp, mu, log_var = vae.forward(dataset[100].to(accelerator.device))\n",
    "z = vae.reparameterize(mu, log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b798a-fab5-41d6-ab89-de52d0b6e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85c033-9954-437b-822e-bcf936ed56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd043511-f3a6-4099-9709-4574c0c59ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf13a06-dca6-4efb-a948-e146dfbb2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify this function to avoid np conversions and differentiation problems\n",
    "def calculate_initial_theta(segment, canvas_size, original_position):\n",
    "    # Theta consists of 6 values, 4 of which we have to calculate.\n",
    "    x_ratio = canvas_size[0] / segment.size[0]\n",
    "    y_ratio = canvas_size[1] / segment.size[1]\n",
    "    # grid_location_x and grid_location_y are basically percentages of height and width and not actual coordinates\n",
    "    # Because we already warp the segment onto a bigger canvas, this transformation is a bit complicated\n",
    "    # grid_location_x has to be in the interval [-(x_ratio-1),(x_ratio-1)], i.e. 0 means -(x_ratio - 1) and 1440 means (x_ratio - 1)\n",
    "    # We can map U ~ [0, 1] to U ~ [a, b] with u -> (a - b)*u + b\n",
    "    # We first map U ~ [0, max_width] to U ~ [0,1] by dividing by max_width\n",
    "\n",
    "    eps = 0.00001 # Avoid div by zero\n",
    "    original_x_position = (original_position[0]) / (canvas_size[0]-segment.size[0]+eps)\n",
    "    mapped_x_position = (-(x_ratio - 1) - (x_ratio - 1))*original_x_position + (x_ratio-1)\n",
    "\n",
    "    original_y_position = (original_position[1]) / (canvas_size[1]-segment.size[1]+eps)\n",
    "    mapped_y_position = (-(y_ratio - 1) - (y_ratio - 1))*original_y_position + (y_ratio-1)\n",
    "\n",
    "    theta = torch.tensor([\n",
    "        [0.0,0.0,0.0],\n",
    "        [0.0,0.0,0.0]\n",
    "    ]).to(accelerator.device)\n",
    "    \n",
    "    theta[0][0] += x_ratio\n",
    "    theta[1][1] += y_ratio\n",
    "    theta[0][2] += mapped_x_position\n",
    "    theta[1][2] += mapped_y_position\n",
    "    \n",
    "    #return np.array([\n",
    "    #    [x_ratio, 0.0    , mapped_x_position],\n",
    "    #    [0.0    , y_ratio, mapped_y_position]\n",
    "    #])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3b2ce-723a-4708-98cc-6cbe4b565105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateNormalPDF(nn.Module):\n",
    "    def __init__(self, mean, covariance, device):\n",
    "        super(MultivariateNormalPDF, self).__init__()\n",
    "        self.mean = mean.to(device)\n",
    "        self.covariance = covariance.to(device)\n",
    "        self.inv_covariance = torch.inverse(covariance).to(device)\n",
    "        self.det_covariance = torch.det(covariance).to(device)\n",
    "        self.dim = mean.size(0)\n",
    "        self.const = torch.sqrt((2 * torch.pi) ** self.dim * self.det_covariance).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        diff = x - self.mean\n",
    "        exp_term = -0.5 * torch.sum(diff @ self.inv_covariance * diff, dim=1)\n",
    "        return torch.exp(exp_term) / self.const\n",
    "\n",
    "class MultivariateNormalLoss(nn.Module):\n",
    "    def __init__(self, mean, covariance, device=\"cpu\"):\n",
    "        super(MultivariateNormalLoss, self).__init__()\n",
    "        self.pdf = MultivariateNormalPDF(mean, covariance,device)\n",
    "        self.pdf = self.pdf.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pdf_values = self.pdf(x)\n",
    "        nll = -torch.log(pdf_values + 1e-8)  # Adding epsilon to prevent log(0)\n",
    "        return nll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ce4dc-880b-4597-bcc0-1fdc08c63414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(torch.nn.Module):\n",
    "    def __init__(self, vae, z, segments, aesthetics_predictor, device):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.z = torch.nn.Parameter(z.to(device))\n",
    "        self.vae = self.vae.eval().to(device)\n",
    "        self.segments = segments\n",
    "        self.device = device\n",
    "        self.aesthetics_predictor = aesthetics_predictor.to(self.device)\n",
    "        \n",
    "        self.canvas_size = (1,3,NORMAL_UI_DIMENSIONS[1],NORMAL_UI_DIMENSIONS[0])\n",
    "        self.mean = torch.zeros(self.z.shape[1]).to(self.device)\n",
    "        print(f\"Mean shape: {self.mean.shape}\")\n",
    "        self.cov = torch.eye(z.shape[1]).to(self.device)\n",
    "        print(f\"Cov shape: {self.cov.shape}\")\n",
    "        self.mn_loss = MultivariateNormalLoss(self.mean, self.cov, self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        decoded = self.vae.decode(self.z)\n",
    "        print(decoded)\n",
    "        decoded_denormalised = decoded * torch.tensor([NORMAL_UI_DIMENSIONS[0],NORMAL_UI_DIMENSIONS[1]]).to(self.device)\n",
    "        segments_on_canvas = []\n",
    "        \n",
    "        for n in range(len(self.segments)):\n",
    "            theta = calculate_initial_theta(self.segments[n],NORMAL_UI_DIMENSIONS,decoded_denormalised[0][n]) \n",
    "            theta = theta.unsqueeze(0).to(self.device)\n",
    "\n",
    "            if self.device == \"cpu\":\n",
    "                grid = F.affine_grid(theta, self.canvas_size).type(torch.FloatTensor)\n",
    "            else:\n",
    "                grid = F.affine_grid(theta, self.canvas_size).type(torch.FloatTensor).to(self.device)\n",
    "            x = F.grid_sample(transform_to_t(self.segments[n]).unsqueeze(0).to(self.device), grid)\n",
    "            segments_on_canvas.append(x)\n",
    "        print(segments_on_canvas[-1].device)\n",
    "        generated_image = stack_alpha_aware(segments_on_canvas)\n",
    "        generated_image = generated_image[:3]\n",
    "\n",
    "        print(f\"Max val: {generated_image.max()}\")\n",
    "        print(f\"generated_image.unsqueeze(0) device: {generated_image.unsqueeze(0).device}\")\n",
    "        score = self.aesthetics_predictor(generated_image.unsqueeze(0))\n",
    "\n",
    "        mn_loss = self.mn_loss(self.z)\n",
    "            \n",
    "        return score, generated_image, mn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac430a25-3f96-44a0-844f-01e188d91459",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomRicoDataset(\"../combined\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696905f-e15b-477d-84bb-50ed700a2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.image_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580336c-6705-4aee-9151-78d4da89e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = 10291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846bfa4-716a-4e28-bf8c-35922d51a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[dataset_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902ed67-ee48-4cdb-a27f-207dd3adf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite lengthy and manual process to retrieve the segments of the first 5 clickable and first 5 non-clickable\n",
    "img_path = test_dataset[dataset_index][1]\n",
    "json_path = test_dataset[dataset_index][1].replace(\".jpg\",\".json\")\n",
    "\n",
    "image = Image.open(img_path)\n",
    "image = image.convert('RGBA')\n",
    "image = image.resize((1440, 2560), Image.Resampling.LANCZOS)\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    image_json = json.load(f)\n",
    "\n",
    "clickable_segments = get_all_bounding_boxes(image_json[\"activity\"][\"root\"], True)\n",
    "reduced_clickable_segments = [\n",
    "    box\n",
    "    for box in clickable_segments\n",
    "    if (prod([box[2]-box[0],box[3]-box[1]]) > 1) and prod([box[2]-box[0],box[3]-box[1]]) < 0.8 * 1440 * 2560\n",
    "]\n",
    "reduced_clickable_segmented_cropped = []\n",
    "for box in reduced_clickable_segments:\n",
    "    anchor_point = (box[0],box[1])\n",
    "    width = box[2]-box[0]\n",
    "    height = box[3]-box[1]\n",
    "    try:\n",
    "        cropped_image = image.crop((box[0],box[1],box[2],box[3]))\n",
    "    except ValueError:\n",
    "        continue\n",
    "    # Only include segments that are less than 90% the size of the original image\n",
    "    if (prod([box[2]-box[0],box[3]-box[1]]) > 1) and prod([box[2]-box[0],box[3]-box[1]]) < 0.8 * 1440 * 2560:\n",
    "        reduced_clickable_segmented_cropped.append(cropped_image)\n",
    "\n",
    "not_clickable_segments = get_all_bounding_boxes(image_json[\"activity\"][\"root\"], False)\n",
    "not_reduced_clickable_segments = [\n",
    "    box\n",
    "    for box in not_clickable_segments\n",
    "    if (prod([box[2]-box[0],box[3]-box[1]]) > 1) and prod([box[2]-box[0],box[3]-box[1]]) < 0.8 * 1440 * 2560\n",
    "]\n",
    "reduced_not_clickable_segmented_cropped = []\n",
    "for box in not_reduced_clickable_segments:\n",
    "    anchor_point = (box[0],box[1])\n",
    "    width = box[2]-box[0]\n",
    "    height = box[3]-box[1]\n",
    "    try:\n",
    "        cropped_image = image.crop((box[0],box[1],box[2],box[3]))\n",
    "    except ValueError:\n",
    "        continue\n",
    "    # Only include segments that are less than 90% the size of the original image\n",
    "    if (prod([box[2]-box[0],box[3]-box[1]]) > 1) and prod([box[2]-box[0],box[3]-box[1]]) < 0.8 * 1440 * 2560:\n",
    "        reduced_not_clickable_segmented_cropped.append(cropped_image)\n",
    "\n",
    "if len(reduced_clickable_segmented_cropped) < 5 or len(reduced_not_clickable_segmented_cropped) < 5:\n",
    "    raise ValueError\n",
    "# return normalised_clickable_boxes[:5], normalised_not_clickable_boxes[:5]\n",
    "segments = reduced_clickable_segmented_cropped[:5]+reduced_not_clickable_segmented_cropped[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f7da9-79a3-4617-898c-efa4cd2e9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,10))\n",
    "a = 1\n",
    "for i in segments:\n",
    "    fig.add_subplot(2,5,a)\n",
    "    plt.imshow(i)\n",
    "    a+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b06237-90f1-4d50-9527-ed0601d90cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "appsthetics_predictor = torch.load(\"../Dataset_estetica/Regressao/model123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482f134-c8dd-419d-b257-1ddc05c6057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae.encode(test_dataset[dataset_index][0].to(accelerator.device))\n",
    "z = vae.reparameterize(mu, log_var)\n",
    "gd = GradientDescent(vae, z, segments,appsthetics_predictor[\"model\"], \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a3d00-06af-4a6d-a160-2cb15d3a1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.0]*z.shape[1])\n",
    "cov = torch.tensor([1.0]*z.shape[1])\n",
    "covariance = torch.diag(cov)\n",
    "loss_fn = MultivariateNormalLoss(mean, covariance, accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4304fbb-cc8b-4976-8ad6-c0fb5a8910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d5b9b-8805-414e-bf73-b77b2f31b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fff9d9-38ad-42d4-ad65-28f9f713ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gd.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef061db6-d696-48aa-bf6f-e40a4e053a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][0][0]+x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145ddb7-32a9-4036-9536-e7424a1bdf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9cb213-df95-4ca1-85fc-e95798c1d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pil_before = transform_t_to_pil(x[1])\n",
    "display(result_pil_before.resize((int(result_pil_before.size[0]*0.25),int(result_pil_before.size[1]*0.25))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b049f5-08df-43a5-918b-055e85cb6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "optimiser_gd = torch.optim.AdamW(gd.parameters(),lr=0.005)\n",
    "for x in range(5):\n",
    "    score, image, mn_loss = gd.forward()\n",
    "    score = score[0][0]\n",
    "    loss = -score\n",
    "    loss += mn_loss / 10.0\n",
    "    print(f\"aesthetics score: {score}, mn_loss: {mn_loss}, sum: {loss}\")\n",
    "    loss.backward()\n",
    "    scores.append(loss.item())\n",
    "    optimiser_gd.step()\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a543e66-7e2d-477c-82d6-31dcfe03363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, img, pdf_loss = gd.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee660b-c9a7-49b2-8b4e-8ed15cb4a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, pdf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1df19a-1db6-428e-980a-355836e62d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pil_after = transform_t_to_pil(img)\n",
    "display(result_pil_after.resize((int(result_pil_after.size[0]*0.25),int(result_pil_after.size[1]*0.25))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f3199-80db-4ecd-9f7c-7b2624f8e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,40))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(result_pil_before)\n",
    "plt.title(\"before\")\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(result_pil_after)\n",
    "plt.title(\"after\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5933409-7371-4486-906d-3a37dc5ac6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da89073-9598-4194-a15d-bd14d85dabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55135096-7c89-44be-98bf-a3624f8f0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11595ecf-09d6-4366-b9b7-593149b50307",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(img_path)\n",
    "boxes, normalised_bboxes, elems = utils.get_first_n_sorted_elements(image,image_json,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820e757-0b4a-46e0-b2e8-f6b288e8af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e94b2d-30e1-4ed2-a178-fe9aa5e5790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22579c-d9b9-4d2f-be81-9b9869021f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,10))\n",
    "a = 1\n",
    "for i in elems:\n",
    "    fig.add_subplot(1,5,a)\n",
    "    plt.imshow(i)\n",
    "    a+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09890dbd-bfa6-4df0-a01b-7b69adb6a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, elems = utils.get_first_n_sorted_elements(image,image_json,5, False)\n",
    "fig = plt.figure(figsize=(50,10))\n",
    "a = 1\n",
    "for i in elems:\n",
    "    fig.add_subplot(1,5,a)\n",
    "    plt.imshow(i)\n",
    "    a+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621d2b9-838c-4773-93f2-9978237e9cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
