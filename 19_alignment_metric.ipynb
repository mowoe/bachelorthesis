{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b47d4d-bf7f-4011-ab6a-9876f90dd634",
   "metadata": {},
   "source": [
    "This notebook contains the code to the final experiment of the thesis. The goal is to align elements such that they have a proper spacing and are aligned on one side. This is the dumbed down version of the original experiment of this thesis.\n",
    "\n",
    "We will use IoU (Intersection over Union) as the metric for proper alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "64dafed6-ce6c-4f3f-825d-87aa50506e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y fastai && pip install -q fastai==1.0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f7cef05b-50a4-4958-9ee1-197f5fe494c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from math import prod\n",
    "from huggingface_hub import hf_hub_download\n",
    "import tqdm\n",
    "transform_t_to_pil = T.ToPILImage()\n",
    "transform_to_t = transforms.Compose([transforms.ToTensor()])\n",
    "import utils\n",
    "from utils import get_all_bounding_boxes, segment, calculate_initial_theta, stack_alpha_aware, AestheticPredictor, get_random_initial_position\n",
    "# from lion_pytorch import Lion\n",
    "from datetime import datetime\n",
    "import random\n",
    "from huggingface_hub import hf_hub_download\n",
    "import timm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "98ccaa52-6282-4ca0-98d7-3445114d5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "56181854-0946-440f-a2f5-010edcdb26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "appsthetics_predictor = torch.load(\"../Dataset_estetica/Regressao/model123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71fc366-4c88-4ab4-ae6c-0d07c67f16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3154ca-c938-4320-abf7-2873f23f32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0c0f14-8f26-4b4e-be96-c8f1d4743075",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('../combined/5373.jpg')\n",
    "im = im.convert('RGBA')\n",
    "im = im.resize((1440, 2560), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "with open('../combined/5373.json', \"r\") as f:\n",
    "    image_json = json.load(f)\n",
    "\n",
    "reduced_segments = [s for s in segment(im,image_json) if (prod(s[0].size) < 0.80*prod(im.size)) and (prod(s[0].size)>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4109bae-f8f3-43ec-87b6-a7a05a535951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 2560)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452c974a-07f8-461f-bb9e-dd5dc174a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_bounding_boxes(item):\n",
    "    all_boxes = []\n",
    "    if \"bounds\" in item.keys() and \"resource-id\" in item.keys():\n",
    "        all_boxes.append((item[\"bounds\"],item[\"resource-id\"]))\n",
    "    if \"children\" in item.keys():\n",
    "        for child in item[\"children\"]:\n",
    "            for box in get_all_bounding_boxes(child):\n",
    "                all_boxes.append(box)\n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2c52ce-7343-4385-bbb3-b255689cdec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGBA size=168x168>, (1272, 84)),\n",
       " (<PIL.Image.Image image mode=RGBA size=848x142>, (112, 373)),\n",
       " (<PIL.Image.Image image mode=RGBA size=888x242>, (112, 571)),\n",
       " (<PIL.Image.Image image mode=RGBA size=444x76>, (112, 917)),\n",
       " (<PIL.Image.Image image mode=RGBA size=327x168>, (1113, 2224)),\n",
       " (<PIL.Image.Image image mode=RGBA size=1440x168>, (0, 2392)),\n",
       " (<PIL.Image.Image image mode=RGBA size=1440x84>, (0, 0))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6ebeff7b-8286-4238-b21a-74ab60a8458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSegmentUIOptimizer(torch.nn.Module):\n",
    "    def __init__(self,segments_and_positions: list, original_image_size: tuple):\n",
    "        super().__init__()\n",
    "        self.original_image_size = original_image_size\n",
    "        self.segments_and_positions = segments_and_positions\n",
    "        self.canvas_size = (1,3,original_image_size[1],original_image_size[0])\n",
    "        print(f\"Canvas size is {self.canvas_size}\")\n",
    "\n",
    "        initial_vector = []\n",
    "        for segment,position in segments_and_positions:\n",
    "            #initial_theta = calculate_initial_theta(segment,original_image_size,position)\n",
    "            # initial_theta = get_random_initial_position(segment,original_image_size,position)\n",
    "            #initial_vector.append([initial_theta[0][2],initial_theta[1][2]])\n",
    "            initial_vector.append([0.0,0.0])\n",
    "\n",
    "        self.coordinates = torch.nn.Parameter(torch.tensor(initial_vector))\n",
    "        print(self.coordinates)\n",
    "        self.background_color = torch.tensor(np.array([1.0,1.0,1.0]),dtype=torch.float)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        segments_on_canvas = []\n",
    "\n",
    "        # Create background image from parameter\n",
    "        bg_col = torch.clamp(self.background_color, min=0, max=1)\n",
    "        #print(f\"Clamped bg to {bg_col}\")\n",
    "        red = torch.tile(bg_col[0],self.original_image_size[::-1])\n",
    "        green = torch.tile(bg_col[1],self.original_image_size[::-1])\n",
    "        blue = torch.tile(bg_col[2],self.original_image_size[::-1])\n",
    "        alpha = torch.tile(torch.tensor(1.0),self.original_image_size[::-1])\n",
    "        background = torch.stack([red,green,blue,alpha]).unsqueeze(0)\n",
    "        #print(background.detach().sum())\n",
    "        #background = torch.tile(self.background_color,self.original_image_size)\n",
    "        \n",
    "        segments_on_canvas.append(background)\n",
    "        \n",
    "        for n in range(len(self.segments_and_positions)):\n",
    "            # We need to calculate the proper ratios, to artificially warp the segment on to a bigger canvas without distorting it (see notebook 01)\n",
    "            x_ratio = self.original_image_size[0] / self.segments_and_positions[n][0].size[0]\n",
    "            y_ratio = self.original_image_size[1] / self.segments_and_positions[n][0].size[1]\n",
    "            \n",
    "            # Affine matrix\n",
    "            theta = [\n",
    "                [x_ratio, 0.0    , 0.0],\n",
    "                [0.0    , y_ratio, 0.0]\n",
    "            ]\n",
    "            theta_tensor = torch.as_tensor(theta)[None]\n",
    "\n",
    "            theta_tensor[0][0][2]+=self.coordinates[n][0]\n",
    "            theta_tensor[0][1][2]+=self.coordinates[n][1]\n",
    "            \n",
    "            # Generate flow field\n",
    "            grid = F.affine_grid(theta_tensor, self.canvas_size).type(torch.FloatTensor)\n",
    "            x = F.grid_sample(transform_to_t(self.segments_and_positions[n][0]).unsqueeze(0), grid)\n",
    "            segments_on_canvas.append(x)\n",
    "\n",
    "\n",
    "\n",
    "        return segments_on_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "580fa25f-750d-457e-a694-eefaac1d4a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas size is (1, 3, 2560, 1440)\n",
      "Parameter containing:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "msUIo_wbg = MultiSegmentUIOptimizer(reduced_segments, im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4e450f61-89d6-4eef-9ff3-8a36f0ba72d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAAGFCAYAAAAPVES/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAASFklEQVR4nO3d63MU9Z7H8U/PJZP7jZCQBLlFhCAXL4hyiHrc9UIpik9AtzyltQ8sn+zW/jM+t1SWUyu6Va61Hh8gKuApVxSMOUAuhFxJgGSAXIe59W8fzIWEBA2g8oW8Xw80lfTM9HT3u3v61z3qOeecANxxgTs9AwAyiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjiBEwghgBI4gRMIIYASOIETCCGAEjQnd6Bn4Lzjn5vn+nZwN3UCAQkOd5d3o2bss9EaMkYlzkAoG7/0Pe3f8OgHsEMQJGECNgxILPGdtOnfo95+O2ODml02nJ3ek5wR3hScFgUJ7sDuBs2rDhV6fxnHML2oRLyqpue4aAxWpq4vKvTrPgGEOhe2bgFfjDpVKpX51m4YWl+QwI/J4WHGNaXMcDfk+MpgJGLPjIuHLlqt9xNgAseABn+PzF33tegHtW/bLaX51mwUfGioqKOb/L3aAdi8UUiUTyI643umE3171zTvF4XOl0WoWFhQqFgpI8OefmfaxzTolEQslkUgUFBQqHw7/4Ojd63Rs9d+b3Tspep8q9r9z9jjd63ELnwbnMddBQKJT9OaVg8JeX1WJxo3V+/TQzeZ4363fXP37mc8732Ovl7mvO/e1OrZNbul6Re7OeJ0Wjo/rww30KhUJ65ZVXtHr1ql987NjYmL766iudPHlS8XhcdXV1euaZP6u5ecO1BZid1pM0PT2tw4cP68SJE5qemlL1kiVqaWnRww8/nLnQO2PB5eYrt/Enk0mFw+E50+WMj4+ru7tbK1euVHf3GUUihfI8T9FoVEND5/Tyy6+otLR0znuXpMHBQbW3t6uxsVHr1q1TMBi84YaVTqd1+vRprXvgAaX9tLq6urRp02bJ8EXqP0JuWfq+r87ODq1evUaRSIEkL7MNZJdnMplUf3+/enrOamxsXKFQSHV1dVq79n5VVy+ZN7xkMqmpqSk55xQIBFRWVjbnZvJEIqH+/n4dPHhQwWBAL7ywU/X19QqFwsqtxms7Xen6A0bmZ8m5azuI2X+7ufV7SzF6niff9zU4OKjGhno99+w/6f3339fHH/+X/u3f/0NFhcWaezuM09TUpD784H11d5+Rk5ROpTQ5Mab+vh69/vq/6OFHHp0xtZRMJHTgo4904sSPkucplUppampCf93fq6nJcT3952d0/QbtnNO5c+fU3d2dn9c1a9aosbFxzoKamJjQwMCA5Dkd++GYEom4ampqdOrkKU1NT+mBdQ/o0Ue2znn/Y2Nj+uS/D2j1qtX68tBBFRQUqKmpad49tCSd6erSD8f+T60/HZfv+/KdU83Spapf1jjPclo8nHM6f35Y1Uuq9OWhg6qvr9eul15WQUFEkuR7Un9vrz779FP19JxVKpVUrjfPk8rKyrRt2+N6/oWdKi4uliRdvHhRoVBI4xNjGh4eknNOwUBQmzZtUUlJyazX//nnVn184IASibg8z1NvX4/efPNf9cDadZLcjJ1FWmfP9qixsVFFRUXZeXBKp1I623NWK1aslOdJnZ1d+W0h98nnZtzSaKpzTqlUSp988om++eZrdXR0KJVKaWRkRFOTk9mZvX7DlH788Ud1dnVp/fr1emjLFkUiEe3atUvJZFJffPGFpqenM9Nm/9ne3q4TJ06ooaFBT7a0KBgI6MUXX1QoFNLBgwcVjV7KL7RcYFeuXFF3d7eam5tVVlam9evXq7u7W1euXJm153TOqbKyUtu3b1dDQ4N27PiTysvL1dXVpcmpSW3fvl0bN26c9/2Pjo4oHA6r5ckWrV61SpcvX84/5/VSqaQ+/9vf1N7erpMnT+r06dP6x8l/6Juvv85OsTiPjvkd1ZluHTx4UJFIRCMjI2ptbc3+LfP3yckJ9ff3K5VKSspEmDkaOY2Pj6uvr2/WKUN1dbWGh4cViURUVlYmSfKdk3P+nPWTSCTknK933nlHb775ptKptFLJpHLrJPPpz1NnZ5fee+89XbhwQfF4XMlkQpL0c1ub3n//A42Pj+nw4SPq6urSd999p7a2Nt3kQVHSbVzaCIVCqq2t1aef/o+OHDmidNpXIBCc8b3C2W/cOafOzi657BG1r69PqVRKra2tSqfTGh0d1aVLl65Nr8wRJZlMKhqNqqOjQ77vq7W1VYlEQuPjExoeHtb1G/Pw8LCWL1+usrIyRaNRlZaWavny5RoeHs4voNwRrLi4WNXV1SouLta3336rtrY2VVRUaNu2bdq9e7cKCwvnvO9UKqWDX36ps2fP6t1339XRo0d1+PA3isVic+bF8zxdvRrXlSuXVVlZpampKcViMVVVVioajcr3524gi4XneQoEAtqx408qLCxUV1eXKsortHHTplnTbNjwoHbu3KlgMJj/fW6RLV26VK+99lr+iOecUzCY2QYHBwZVU1OjsrKy7Pqe71NLJt4VK1aop7c3O4k36zUmJyd17Ngx1dTUyPM8/fTTT+rs7NTY2Jja2tpUU1Mj33fq6+vTtm3btHnzpvynspt1SzHmFuTOnTu1ectDqqis0tLaWsWmp/XxgY80Pp45Ys2nsKhIsVhMo6OjSqfT6unpUXl5+ZyT8uwLKRKJyDmn4eFhOefU29urkpISBQLenNfIHSFzAzDpdDr/++wTznoPUuZLqZcvX1ZBQUH2o/c5bdn8kKSA4lcTc+bf931duXxZvu+rrq5OgWBAV8auZF9rnt2hc3Lu2rxJuT314owwJ/f+A4GASktL5fu+ioqKFMpH58lzUsDz9NTTT2nbtsevPdjzVFxSpj17X1fdsvr88808TUgmkxoYGFBNTY1KS0pvuLwnJibU1tamLZs3q7CwSDO3Keecjhw5ooaGehVGIpqamtLWrVu1bt06ffXVV1qzZo2CwaAmJyfyA365A9KtrN/buuG0urpab731lq5ejUmSPv/8f/Xt0aP6z3379Je/vKnyGSOwgUBAa9euVWdHu9Y0Nen8+fNKJBKqqalROp1WSUmpampq8tN7ku5vatLfv/1Wq1at0sjIiKanp1VVVaWCggJ5XkAN9Q3Xps+uiPr6erW1tam8vFy1tUs1OTmpwcFBbd68OT/tzBXX19enD/d9oFhsWg8++KCSiZRGRkZ1+nS7Kioq9Nxzz817Il5UVKQdO3YoFovNOKLP3UFECgtVUVGh3p7M3jKdTuv8+fNas+b+GQMKiy/M3M73+PHj6u/vV9OaNTp/4bxaW1v16KNb84M3TlI4XKDdu19RNDqqjo4OhcJh7dq1S83NzXMGTmbKBbnivhWKRCLzTjM5OanJyUkdOnRIo6Ojs/6WTqeUTCY1NjamiyMj6urq0n33Lc/vXAcHBzU6Oqru7rMqKSlWNBrVpUuXVFlZKenmB3Fu6w4cz/MUCoVUUlKqkpJSvbp7t1paWtTR0aH9f92vRCI+a/qtW7dqxYoV6u7uVnFxiZYuXaqxsTFFo1G9sPOF/El4zvrmZm3cuFFnzpxROBxWbW2t4vG4hoaG9Oyz/6zqJUvy85HbE1VVVampqUnt7e2amJjU6dOn1dTUpMrKylnnFs45DQwMaN++fbpw4YLWr1+vN954Q0888YQGBwcVj8dVVVU167lz/w4XFOjq1avav3+/BgcHFQ4XzLt8nHMKh8N6/vnn1dzcrIaGBjU2NuqhLQ/pyZaW7LwsvhCvcSovL9dLL76ktO+ruLhY69c3z5nKk1RSWqo9e/dq2bJlatnRou3bt+djnbnBe56ncDgsLxBQIBBQOp3WuaGh/Mjq9SKRiI4ePaqBgYHcmaKkzChpKBTWyy+/rL179+qxxx7T448/ro6OTg0NDenVV1/Vnj17tPXRR7Vt22N66qmndfz4cQ0NDWnr1rmDfgux4Iv+07H4L/49N8wbj8f12WefaWBgUG+//bZKS0t07YiRGWA5dOhLnTp1Sol4QrUzLm0EAoHMAvOuLZbMpY1vdPz4CcVi06qqqtaTTz6pRx55eNaI1czLGrnh8NyljZnfOJk5onrp0iUdOHBAkUhEe/fuUXFxsX788bhOnTqlhoYGtbTsUCRSOOtxvu/r3LlzisevZp9HCofDWr58uYLB0JwT95kjcqlUOruSQ/K8gCTvlk707xW5bcb3ff3www/asGGDSkpK5x7tsqOXTtLw8JAqK6tUVFQkaf5rjKlUSolEYtYHlVgspiVLlsyavq+vTydOHM8+TgoEPD3xxHbV1tbNubQxa/uS5GU/1fh+OrsuMz9LXv78duZ7KC6K/Ory+M1inClzjS+Rv253Peecrl69Kt/3Z90scCOZi/5xJZMpRSKR/EX/25WbD0nZIWunixcvKpVKqa6u7qa+NnYr15Uw/wj0Ly3H3245Zw4OC7kp4Lfwm8bI7XDArVvI7XALjnHVqlW3Oz/AotXb2/ur0yw4Rj6CAbduIZnxfUbACGIEjCBGwIgFj93Pd58mgN/OgmP8/vvvf8/5uC257y865xRPpxfhzWWLk+ecCkIhBb3shfa7fJBxwaOpluXuukgmkzofvcR/VXKRCMjXspoaFYTCCoVDd/2I/z0VY+bnOzwz+GN5Tp687C2Gd3eM99x/JvwuXx+4affOCmc0FTDinjky3gv/51osbvfEOSNwL+BwAhhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIARxAgYQYyAEcQIGEGMgBHECBhBjIAR/w/+5RkY2t+ebgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "canvasses = msUIo_wbg()\n",
    "\n",
    "\n",
    "generated_image = stack_alpha_aware(canvasses)\n",
    "# Remove Alpha channel\n",
    "generated_image = generated_image[:3]\n",
    "\n",
    "plt.imshow(generated_image.detach().permute(1, 2, 0))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e3c7386a-0fb9-4655-a14f-3a4233fabb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "    boxA (tuple): A tuple (x1, y1, x2, y2) representing the first bounding box.\n",
    "    boxB (tuple): A tuple (x1, y1, x2, y2) representing the second bounding box.\n",
    "    \n",
    "    Returns:\n",
    "    float: IoU value between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interWidth = max(0, xB - xA)\n",
    "    interHeight = max(0, yB - yA)\n",
    "    interArea = interWidth * interHeight\n",
    "\n",
    "    # Compute the area of both the bounding boxes\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    # Compute the area of the union\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = interArea / float(unionArea)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def get_xy(canvas_size, segment, coordinates):\n",
    "    eps = 0.00001 # Avoid div by zero\n",
    "    \n",
    "    x_ratio = (canvas_size[0] / segment.size[0]) + eps\n",
    "    y_ratio = (canvas_size[1] / segment.size[1]) + eps\n",
    "        \n",
    "    original_position = [0,0]\n",
    "    mapped_x_position, mapped_y_position = coordinates[0], coordinates[1]\n",
    "\n",
    "    original_x_position = ((mapped_x_position  -(x_ratio-1)) / (-(x_ratio - 1) - (x_ratio - 1)))\n",
    "    \n",
    "    original_position[0] = original_x_position * (im.size[0]-segment.size[0]+eps)\n",
    "\n",
    "    original_y_position = ((mapped_y_position -(y_ratio-1)) / (-(y_ratio - 1) - (y_ratio - 1))) \n",
    "    original_position[1] = original_y_position * (im.size[1]-segment.size[1]+eps)\n",
    "\n",
    "    size = segment.size\n",
    "    x1,y1,x2,y2 = [original_position[0],original_position[1],original_position[0]+size[0],original_position[1]+size[1]]\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def criterion(coordinates,segments,canvas_size):\n",
    "    ious = []\n",
    "        \n",
    "    for second_segment in range(len(coordinates)):\n",
    "        second_xy = get_xy(canvas_size, segments[second_segment], coordinates[second_segment])\n",
    "        for segment in range(len(coordinates)):\n",
    "            if segment == second_segment:\n",
    "                continue\n",
    "                # pass\n",
    "            xy = get_xy(canvas_size, segments[segment], coordinates[segment])\n",
    "            ious.append(calculate_iou(xy,second_xy))\n",
    "    \n",
    "    return sum(ious) / len(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c91706ea-3ebe-4e6c-b034-e9b66bd54c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msUIo_wbg.coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "29fb4cb4-f0e9-4c71-8677-d5d2ffebf9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3018, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(msUIo_wbg.coordinates, [x[0] for x in reduced_segments], im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1ea44ed8-0df3-43a1-8c90-d5192d564e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in reduced_segments][0].size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "93be133e-0631-401a-87c5-07193a545220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 168)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_segments[0][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "26159c52-d0f9-4dd4-89b6-b529de6acb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(msUIo_wbg.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8ad12084-a3bf-4768-9ebc-e95a4022e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesthetics Score: 0.4470492899417877 (Loss: 0.5529507100582123)\n",
      "IoU score: 0.30176979303359985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:03<06:28,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesthetics Score: 0.5004582405090332 (Loss: 0.4995417594909668)\n",
      "IoU score: 0.30176979303359985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:24,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesthetics Score: 0.5445227026939392 (Loss: 0.4554772973060608)\n",
      "IoU score: 0.2992900311946869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:11<06:14,  3.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Remove Alpha channel\u001b[39;00m\n\u001b[1;32m     14\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m generated_image[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mappsthetics_predictor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAesthetics Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m iou_score \u001b[38;5;241m=\u001b[39m criterion(msUIo_wbg\u001b[38;5;241m.\u001b[39mcoordinates, [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m reduced_segments], im\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/models/resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "random_losses = []\n",
    "combined_losses = []\n",
    "images_for_gif = []\n",
    "ious = []\n",
    "\n",
    "\n",
    "for x in tqdm.tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    canvasses = msUIo_wbg()\n",
    "    \n",
    "    generated_image = stack_alpha_aware(canvasses)\n",
    "    # Remove Alpha channel\n",
    "    generated_image = generated_image[:3]\n",
    "\n",
    "    score = appsthetics_predictor[\"model\"](generated_image.unsqueeze(0))\n",
    "    print(f\"Aesthetics Score: {score.detach().item()} (Loss: {1- score.detach().item()})\")\n",
    "\n",
    "    iou_score = criterion(msUIo_wbg.coordinates, [x[0] for x in reduced_segments], im.size)\n",
    "    print(f\"IoU score: {iou_score}\")\n",
    "    ious.append(iou_score)\n",
    "    loss = iou_score\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "    scores.append(score.detach().item())\n",
    "\n",
    "    # Save image as png to create gif\n",
    "    pil_image = transform_t_to_pil(generated_image.detach())\n",
    "    pil_image = pil_image.resize((int(pil_image.size[0]/4),int(pil_image.size[1]/4)), Image.Resampling.LANCZOS)\n",
    "    pil_image.save(f\"output/{x}.png\")\n",
    "    images_for_gif.append(f\"output/{x}.png\")\n",
    "    #pbar.set_description(f\"Score: {score}\")\n",
    "    #pbar.refresh()\n",
    "\n",
    "print(f\"Final Score: {score[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fe65c208-fc65-4af6-9502-3b8844fa39b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317],\n",
       "        [-1.1317, -1.1317]], requires_grad=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msUIo_wbg.coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fce31c-9411-4aae-9882-876d483227ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
